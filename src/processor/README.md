# ECS Processor - Gerador de Documentos TÃ©cnicos

Processador ECS Fargate que transforma transcriÃ§Ãµes de vÃ­deos em documentos tÃ©cnicos estruturados de treinamento e troubleshooting usando AWS Bedrock (Claude Sonnet 4).

---

## ğŸ“‹ Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Arquitetura](#arquitetura)
3. [Pipeline de 6 Etapas](#pipeline-de-6-etapas)
4. [Componentes](#componentes)
5. [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
6. [Uso](#uso)
7. [Desenvolvimento Local](#desenvolvimento-local)
8. [Deploy](#deploy)
9. [Monitoramento](#monitoramento)
10. [Troubleshooting](#troubleshooting)

---

## VisÃ£o Geral

### Responsabilidades

O processador ECS Ã© responsÃ¡vel por:

1. **Carregar transcriÃ§Ãµes** do S3 (formato JSON do AWS Transcribe)
2. **Chunking adaptativo** para vÃ­deos longos (>200K tokens)
3. **Processar via LLM** usando Claude Sonnet 4 na AWS Bedrock
4. **Gerar documentos** em Markdown e DOCX
5. **Upload no S3** e atualizaÃ§Ã£o do DynamoDB

### CaracterÃ­sticas

- âœ… Pipeline de 6 etapas granulares
- âœ… Suporte a transcriÃ§Ãµes longas (atÃ© 3 horas de vÃ­deo)
- âœ… Chunking inteligente com overlap
- âœ… Rate limiting e retry logic
- âœ… Multi-formato: Markdown + DOCX
- âœ… Tracking completo de custos e tokens
- âœ… Error handling robusto

---

## Arquitetura

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           main.py                       â”‚
â”‚   (Entry Point & Orchestration)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              â”‚              â”‚             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
    â”‚transcriptionâ”‚  â”‚llm_client â”‚  â”‚document_   â”‚ â”‚__init__ â”‚
    â”‚  _parser    â”‚  â”‚           â”‚  â”‚ generator  â”‚ â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

```
S3 Transcription â†’ Parse â†’ Chunk â†’ Stage 1-5 (LLM) â†’ Stage 6 (Output) â†’ S3
                                                                         â†“
                                                                    DynamoDB
```

---

## Pipeline de 6 Etapas

### Etapa 1: Limpeza da TranscriÃ§Ã£o

- Parse do JSON do AWS Transcribe
- FormataÃ§Ã£o com timestamps e speakers
- RemoÃ§Ã£o de ruÃ­do (conversas nÃ£o tÃ©cnicas)

**Input**: JSON do Transcribe  
**Output**: Texto limpo formatado  
**LLM**: NÃ£o (processamento local)

### Etapa 2: ExtraÃ§Ã£o de ConteÃºdo TÃ©cnico

- IdentificaÃ§Ã£o de erros e diagnÃ³sticos
- ExtraÃ§Ã£o de soluÃ§Ãµes e comandos
- Riscos de ambiente
- Regras de negÃ³cio
- ConfiguraÃ§Ãµes

**Input**: Texto limpo  
**Output**: JSON estruturado  
**LLM**: Sim (Claude Sonnet 4)

### Etapa 3: Mapeamento de SoluÃ§Ãµes

- CriaÃ§Ã£o de matriz problema â†’ soluÃ§Ã£o
- Medidas preventivas
- Passos de debugging

**Input**: JSON tÃ©cnico da Etapa 2  
**Output**: JSON com mapeamentos  
**LLM**: Sim (Claude Sonnet 4)

### Etapa 4: EstruturaÃ§Ã£o do Documento

- CriaÃ§Ã£o do outline do documento
- OrganizaÃ§Ã£o em seÃ§Ãµes lÃ³gicas
- DefiniÃ§Ã£o de estrutura (sem conteÃºdo)

**Input**: JSON de soluÃ§Ãµes da Etapa 3  
**Output**: Outline estruturado  
**LLM**: Sim (Claude Sonnet 4)

### Etapa 5: RedaÃ§Ã£o do ConteÃºdo

- Escrita completa do documento em Markdown
- Tom profissional e didÃ¡tico
- FormataÃ§Ã£o rica (code blocks, tabelas, listas)

**Input**: Outline da Etapa 4  
**Output**: Documento Markdown completo  
**LLM**: Sim (Claude Sonnet 4, max_tokens=8192)

### Etapa 6: GeraÃ§Ã£o de Outputs

- Salvar Markdown no S3
- ConversÃ£o Markdown â†’ DOCX (python-docx)
- Salvar DOCX no S3
- ValidaÃ§Ã£o de outputs

**Input**: Markdown da Etapa 5  
**Output**: Arquivos `.md` e `.docx` no S3  
**LLM**: NÃ£o (processamento local)

---

## Componentes

### 1. `transcription_parser.py`

**Classe Principal**: `TranscriptionParser`

**Funcionalidades**:
- Parse de JSON do AWS Transcribe
- IdentificaÃ§Ã£o de speakers
- ExtraÃ§Ã£o de timestamps
- Chunking adaptativo para transcriÃ§Ãµes longas
- Contagem de tokens

**Exemplo de Uso**:

```python
from transcription_parser import TranscriptionParser

parser = TranscriptionParser(max_tokens_per_chunk=100000)

# Parse JSON
parsed = parser.parse_transcribe_json(json_data)

# Chunk se necessÃ¡rio
chunks = parser.chunk_transcription(parsed)
print(f"Created {len(chunks)} chunk(s)")
```

### 2. `llm_client.py`

**Classe Principal**: `BedrockLLMClient`

**Funcionalidades**:
- Cliente LangChain para AWS Bedrock
- Retry com exponential backoff
- Rate limiting (10 req/min, 100K tokens/min)
- Streaming support
- Token tracking e cÃ¡lculo de custos

**Exemplo de Uso**:

```python
from llm_client import BedrockLLMClient

client = BedrockLLMClient(
    model_id="anthropic.claude-sonnet-4-5-20250929-v1:0",
    temperature=0.7,
    max_tokens=4096
)

# Invocar modelo
response, usage = client.invoke(prompt)
print(f"Tokens: {usage.total_tokens}, Cost: ${usage.calculate_cost()}")

# Com JSON output
json_response, usage = client.invoke_with_json_output(prompt)
```

### 3. `document_generator.py`

**Classe Principal**: `DocumentGenerator`

**Funcionalidades**:
- OrquestraÃ§Ã£o completa do pipeline
- Processamento single-chunk e multi-chunk
- GeraÃ§Ã£o de Markdown e DOCX
- Merge de resultados de chunks

**Exemplo de Uso**:

```python
from document_generator import DocumentGenerator

generator = DocumentGenerator(llm_client, parser, s3_client)

result = generator.generate_document(
    execution_id="uuid-123",
    transcription_s3_uri="s3://bucket/transcription.json",
    output_bucket="output-bucket"
)

print(f"Document generated: ${result.total_cost_usd:.4f}")
print(f"Markdown: {result.markdown_s3_uri}")
print(f"DOCX: {result.docx_s3_uri}")
```

### 4. `main.py`

**FunÃ§Ã£o Principal**: `lambda_handler(event, context)`

**Responsabilidades**:
- Entry point do ECS task
- ConfiguraÃ§Ã£o e validaÃ§Ã£o
- InicializaÃ§Ã£o de componentes
- OrquestraÃ§Ã£o do fluxo
- Error handling
- Update do DynamoDB

---

## ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

**ObrigatÃ³rias**:

| VariÃ¡vel | DescriÃ§Ã£o | Exemplo |
|----------|-----------|---------|
| `TRACKING_TABLE` | Nome da tabela DynamoDB | `ai-techne-academy-tracking-dev` |
| `OUTPUT_BUCKET` | Bucket S3 para outputs | `ai-techne-academy-output-dev` |

**Opcionais** (com defaults):

| VariÃ¡vel | DescriÃ§Ã£o | Default |
|----------|-----------|---------|
| `AWS_REGION` | RegiÃ£o AWS | `us-east-1` |
| `BEDROCK_MODEL_ID` | ID do modelo Bedrock | `anthropic.claude-sonnet-4-5-20250929-v1:0` |
| `LOG_LEVEL` | NÃ­vel de log | `INFO` |
| `MAX_TOKENS_PER_CHUNK` | Max tokens por chunk | `100000` |

### IAM Permissions

O ECS Task Role precisa das seguintes permissÃµes:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::input-bucket/*",
        "arn:aws:s3:::transcription-bucket/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:PutObject"
      ],
      "Resource": [
        "arn:aws:s3:::output-bucket/*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "bedrock:InvokeModel",
        "bedrock:InvokeModelWithResponseStream"
      ],
      "Resource": [
        "arn:aws:bedrock:*::foundation-model/anthropic.claude-sonnet-4-5-*"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "dynamodb:GetItem",
        "dynamodb:PutItem",
        "dynamodb:UpdateItem"
      ],
      "Resource": [
        "arn:aws:dynamodb:*:*:table/tracking-table-name"
      ]
    }
  ]
}
```

---

## Uso

### Event Format (Input)

```json
{
  "execution_id": "550e8400-e29b-41d4-a716-446655440000",
  "video_s3_uri": "s3://input-bucket/video.mp4",
  "transcription_s3_uri": "s3://transcription-bucket/uuid/transcription.json",
  "video_metadata": {
    "duration": 10800,
    "size_bytes": 2147483648
  }
}
```

### Response Format (Output)

**Sucesso** (statusCode: 200):

```json
{
  "statusCode": 200,
  "body": {
    "execution_id": "550e8400-e29b-41d4-a716-446655440000",
    "markdown_s3_uri": "s3://output-bucket/uuid/document.md",
    "docx_s3_uri": "s3://output-bucket/uuid/document.docx",
    "tokens_used": {
      "input": 150000,
      "output": 8000,
      "total": 158000
    },
    "cost_usd": 0.57,
    "duration_seconds": 185,
    "chunks_processed": 2,
    "stages_completed": 6
  }
}
```

**Erro** (statusCode: 400/500):

```json
{
  "statusCode": 500,
  "body": {
    "error": "ProcessingError",
    "message": "Failed to invoke Bedrock: ThrottlingException"
  }
}
```

---

## Desenvolvimento Local

### 1. Instalar DependÃªncias

```bash
cd src/processor
pip install -r requirements.txt
```

### 2. Configurar Ambiente

```bash
export TRACKING_TABLE=ai-techne-academy-tracking-dev
export OUTPUT_BUCKET=ai-techne-academy-output-dev
export AWS_REGION=us-east-1
export AWS_PROFILE=your-profile
```

### 3. Executar Localmente

```bash
python main.py '{
  "execution_id": "test-123",
  "video_s3_uri": "s3://bucket/video.mp4",
  "transcription_s3_uri": "s3://bucket/transcription.json"
}'
```

### 4. Teste com JSON File

```bash
python main.py "$(cat test-event.json)"
```

---

## Deploy

### ğŸ³ Docker

O processador Ã© containerizado usando Docker multi-stage build para otimizaÃ§Ã£o de tamanho e performance.

#### Dockerfile

O [`Dockerfile`](./Dockerfile) implementa:
- âœ… Multi-stage build (builder + runtime)
- âœ… Python 3.12 slim
- âœ… OtimizaÃ§Ã£o de camadas com cache
- âœ… Imagem final ~200-300MB

#### Build Local

Use o script automatizado:

```bash
# Build da imagem
./scripts/build-processor.sh

# Ou manualmente
cd src/processor
docker build -t ai-techne-processor:latest .
```

#### Desenvolvimento Local com Docker Compose

O [`docker-compose.yml`](./docker-compose.yml) fornece ambiente completo para desenvolvimento:

```bash
# Iniciar container em background
cd src/processor
docker-compose up -d

# Ver logs
docker-compose logs -f

# Executar comandos no container
docker-compose exec processor python -c "import boto3; print('âœ“ AWS SDK loaded')"

# Parar e remover
docker-compose down
```

**ConfiguraÃ§Ã£o**:
```yaml
services:
  processor:
    build: .
    environment:
      - AWS_REGION=us-east-1
      - TRACKING_TABLE=ai-techne-academy-tracking-dev
      - OUTPUT_BUCKET=ai-techne-academy-output-dev-<account-id>
    volumes:
      - ~/.aws:/root/.aws:ro  # AWS credentials
      - ./:/app               # Hot reload
    resources:
      limits:
        cpus: '2.0'
        memory: 8G
```

#### Push para ECR

**PrÃ©-requisito**: RepositÃ³rio ECR criado via SAM template

```bash
# Deploy da stack SAM (cria ECR repository)
cd infrastructure
sam deploy --guided

# Push da imagem usando script automatizado
./scripts/push-processor.sh
```

O script [`push-processor.sh`](../../scripts/push-processor.sh) automatiza:
1. âœ… Login no ECR
2. âœ… Tag da imagem (latest + timestamp)
3. âœ… Push para ECR
4. âœ… VerificaÃ§Ãµes de seguranÃ§a

**URI da Imagem**:
```
<account-id>.dkr.ecr.us-east-1.amazonaws.com/ai-techne-academy/processor:latest
```

#### Teste do Container

```bash
# Teste de dependÃªncias
docker run --rm ai-techne-processor:latest \
  python -c "import boto3, langchain, docx; print('âœ“ All dependencies loaded')"

# Teste com event mock (com AWS credentials)
docker run --rm \
  -v ~/.aws:/root/.aws:ro \
  -e AWS_REGION=us-east-1 \
  -e TRACKING_TABLE=test-table \
  -e OUTPUT_BUCKET=test-bucket \
  ai-techne-processor:latest \
  python main.py '{"execution_id":"test-123","video_s3_uri":"s3://bucket/video.mp4","transcription_s3_uri":"s3://bucket/transcript.json"}'
```

### ğŸ“¦ ECR Repository

O repositÃ³rio ECR Ã© gerenciado via SAM template ([`template.yaml`](../../infrastructure/template.yaml)):

```yaml
ProcessorRepository:
  Type: AWS::ECR::Repository
  Properties:
    RepositoryName: ai-techne-academy/processor
    ImageScanningConfiguration:
      ScanOnPush: true
    LifecyclePolicy:
      # MantÃ©m Ãºltimas 5 imagens
      # Expira untagged apÃ³s 7 dias
```

**Lifecycle Policy**:
- MantÃ©m Ãºltimas 5 imagens tagged
- Remove imagens untagged apÃ³s 7 dias
- Scan automÃ¡tico de vulnerabilidades

### ğŸš€ ECS Task Definition

Para criar ECS Task Definition usando a imagem:

```json
{
  "family": "ai-techne-processor",
  "taskRoleArn": "arn:aws:iam::<account>:role/ai-techne-academy-ecs-task-dev",
  "executionRoleArn": "arn:aws:iam::<account>:role/ai-techne-academy-ecs-execution-dev",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "2048",
  "memory": "8192",
  "containerDefinitions": [{
    "name": "processor",
    "image": "<account>.dkr.ecr.us-east-1.amazonaws.com/ai-techne-academy/processor:latest",
    "logConfiguration": {
      "logDriver": "awslogs",
      "options": {
        "awslogs-group": "/ecs/ai-techne-academy-processor-dev",
        "awslogs-region": "us-east-1",
        "awslogs-stream-prefix": "ecs"
      }
    },
    "environment": [
      {"name": "TRACKING_TABLE", "value": "ai-techne-academy-tracking-dev"},
      {"name": "OUTPUT_BUCKET", "value": "ai-techne-academy-output-dev-<account>"},
      {"name": "AWS_REGION", "value": "us-east-1"},
      {"name": "LOG_LEVEL", "value": "INFO"}
    ]
  }]
}
```

---

## Monitoramento

### CloudWatch Logs

Logs sÃ£o enviados para `/ecs/ai-techne-academy-processor-dev`

**Exemplo de Log**:

```
2024-12-11 14:00:00 - main - INFO - Starting document generation for execution abc-123
2024-12-11 14:00:01 - transcription_parser - INFO - Parsing AWS Transcribe JSON output
2024-12-11 14:00:02 - transcription_parser - INFO - Parsed: 150000 chars, 1200 segments, 3 speakers
2024-12-11 14:00:02 - transcription_parser - INFO - Chunking transcription: 180000 tokens
2024-12-11 14:00:02 - transcription_parser - INFO - Creating 2 chunks (~90000 tokens each)
2024-12-11 14:01:15 - llm_client - INFO - Bedrock response: 2500 chars, 8000 tokens, $0.135
2024-12-11 14:03:05 - document_generator - INFO - Stage 5 complete: 28000 tokens, 35000 chars
2024-12-11 14:03:10 - main - INFO - Document generation complete: 185.3s, $0.57, 35000 chars
```

### MÃ©tricas

- **Tokens processados**: Total input + output
- **Custo por execuÃ§Ã£o**: USD
- **DuraÃ§Ã£o**: Segundos
- **Chunks processados**: NÃºmero de chunks
- **Taxa de sucesso**: % de execuÃ§Ãµes bem-sucedidas

---

## Troubleshooting

### Erro: "ThrottlingException"

**Causa**: Rate limit do Bedrock excedido (10 req/min ou 100K tokens/min)

**SoluÃ§Ã£o**:
- Rate limiter jÃ¡ implementado
- Se persistir, solicitar aumento de quota AWS
- Reduzir `MAX_TOKENS_PER_CHUNK`

### Erro: "OutOfMemoryError"

**Causa**: TranscriÃ§Ã£o muito grande para memÃ³ria disponÃ­vel

**SoluÃ§Ã£o**:
- Aumentar memÃ³ria do ECS task (de 8GB para 16GB)
- Reduzir `MAX_TOKENS_PER_CHUNK` para mais chunks menores

### Erro: "DynamoDB UpdateItem failed"

**Causa**: PermissÃµes insuficientes ou table nÃ£o existe

**SoluÃ§Ã£o**:
- Verificar IAM role do ECS task
- Confirmar nome da tabela em `TRACKING_TABLE`
- Verificar regiÃ£o AWS

### Documento Gerado Incompleto

**Causa**: `max_tokens` insuficiente na Stage 5

**SoluÃ§Ã£o**:
- Stage 5 usa `max_tokens=8192` (jÃ¡ aumentado)
- Se necessÃ¡rio, editar `document_generator.py` linha ~600

---

## Custos

### Estimativa por ExecuÃ§Ã£o

**VÃ­deo 3 horas** (~270K palavras â†’ ~360K tokens):

| Componente | Custo |
|------------|-------|
| Input tokens (~150K) | $0.45 |
| Output tokens (~10K) | $0.15 |
| **Total Bedrock** | **~$0.60** |
| S3 + DynamoDB | < $0.01 |
| ECS Fargate (3min) | $0.01 |
| **Total** | **~$0.62** |

### OtimizaÃ§Ã£o

- âœ… Chunking inteligente reduz tokens processados
- âœ… Rate limiting evita custos de retry
- âœ… Cache de resultados (se implementado)

---

## PrÃ³ximos Passos

- [ ] Implementar testes unitÃ¡rios
- [ ] Adicionar cache de resultados intermediÃ¡rios
- [ ] Melhorar conversÃ£o Markdown â†’ DOCX (formataÃ§Ã£o avanÃ§ada)
- [ ] Suporte a templates DOCX customizados
- [ ] MÃ©tricas CloudWatch customizadas
- [ ] Streaming de progresso via WebSocket

---

## Links Relacionados

- [Design TÃ©cnico Completo](../../docs/PROCESSOR_DESIGN.md)
- [EspecificaÃ§Ã£o do Projeto](../../docs/SPECIFICATION.md)
- [Lambda Functions](../functions/)
- [Infraestrutura SAM](../../infrastructure/)

---

**VersÃ£o**: 1.0.0  
**Ãšltima AtualizaÃ§Ã£o**: 2024-12-11  
**Autor**: AI Techne Academy Team